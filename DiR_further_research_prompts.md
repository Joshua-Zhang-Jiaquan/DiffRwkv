# Enhanced DiR Research Prompts (w/ Tensor Shapes & Implementation Details)\n\n> **Updated 2026-01-25**: Concrete tensor shapes from `rwkv_diffusion_rnn/models/rwkv_denoiser.py` + implementation priorities\n\n## Priority 1: Causality Adaptation (Critical for Quality)\n\n### Prompt 1 — Block-Causal RWKV: Concrete Implementation\n**Goal**: Implement **Option C (block-causal)** from DiR report using existing `MultimodalRWKVDenoiser`. Process text in blocks of size `B=64`, with intra-block bidirectional processing but strict left→right across blocks.\n\n**Tensor shapes**:\n- `text_tokens`: `[B, N]` (N=512 typical)\n- `blocks`: `N//B` blocks of shape `[B, 64]`\n- `rwkv_state_f`: `[B, state_dim]` (forward across blocks)\n- `h_f, h_b`: `[B, 64, D]` per block (D=1024 for RWKV-v7)\n\n**Deliverables**:\n```python\n# Expected interface in rwkv_denoiser.py\nclass BlockCausalRWKV(nn.Module):\n    def forward(self, blocks, state_in, rwkv_f, rwkv_b):\n        # blocks: List[torch.Tensor] of shape [B, block_size]\n        states_out = state_in\n        fused_h = []\n        for block_tokens in blocks:\n            h_f, states_out = rwkv_f(block_tokens, states_out)  # [B,64,D], [B,state_dim]\n            h_b, _ = rwkv_b(reverse(block_tokens), None)        # [B,64,D]\n            h_b = reverse(h_b)\n            h_fused = self.fuse(h_f, h_b)                      # [B,64,D]\n            fused_h.append(h_fused)\n        return torch.cat(fused_h, dim=1), states_out\n```\n1. **State handling**: Show how `rwkv_state` flows **only forward** across blocks\n2. **Fusion**: Compare `torch.cat([h_f;h_b], dim=-1) @ W_proj` vs `h_f * sigmoid(Wg) + h_b * (1-sigmoid(Wg))`\n3. **Training code**: Modify `train_rwkv.py` training loop for block processing\n4. **Overhead**: FLOPs table for `B∈{32,64,128}` vs full bidirectional\n\n### Prompt 2 — Bi-RWKV Fusion Ablation (Tensor-Level)\n**Goal**: Benchmark 3 fusion methods for full bidirectional RWKV on validation set.\n\n**Current prefix fusion** (from `rwkv_denoiser.py:150-154`):\n```python\nprefix_emb = self.latent_projector(latents)  # [B, prefix_len=1, D=1024]\nprefix_type = self.token_type_embed(0)       # [B,1,D]\ncombined = torch.cat([prefix_emb, text_emb]) # [B, N+1, D]\n```\n\n**Extend to Bi-RWKV**:\n```python\nh_forward  = rwkv_f(tokens)    # [B,N,D] \nh_backward = reverse(rwkv_b(reverse(tokens))) # [B,N,D]\n# Fusion 1: Concat\nh_fused = torch.cat([h_forward, h_backward], dim=-1) @ W_proj  # [B,N,2D]→[B,N,D]\n# Fusion 2: Gated\ngate = torch.sigmoid(linear(torch.cat([h_forward, h_backward], dim=-1))) # [B,N,1]\nh_fused = gate * h_forward + (1-gate) * h_backward\n```\n**Metrics**: Latent reconstruction MSE + text perplexity on held-out Qwen32d data\n\n## Priority 2: Conditioning Improvements\n\n### Prompt 3 — AdaLN-Zero Integration (vs Current FiLM)\n**Current** (`rwkv_denoiser.py:120-125`):\n```python\ngamma_beta = self.time_film(cond)  # [B,2D]\ngamma, beta = gamma_beta.chunk(2, dim=-1)  # [B,D] each\nreturn x * (1 + gamma[:,None,:]) + beta[:,None,:]  # FiLM\n```\n\n**Target**: Replace with **AdaLN-Zero** from DiR report §2:\n```python\n# Per-layer MLPs → [B,4*D]: gamma_time, beta_time, gamma_ch, beta_ch\ndef dir_block(x, state, t_emb):\n    # TimeMix path\n    x_norm = F.layer_norm(x, (x.shape[-1],))  # [B,N,D] → [B,N,D]\n    x1 = x_norm * (1 + gamma_time) + beta_time  # [B,N,D]\n    y, state = timemix(x1, state)\n    x = x + y\n    # ChannelMix path (similar)\n    return x, state\n```\n**Deliverables**: Drop-in replacement for `MultimodalRWKVDenoiser._apply_time_conditioning()` + training stability comparison\n\n## Priority 3: Latent Space & Training Stability\n\n### Prompt 4 — Embedding Diffusion Baseline (vs Current Latent Prefix)\n**Current**: 32-dim Qwen latents → 1 RWKV prefix token\n**Research**: Full embedding diffusion per DiR report §3\n\n```python\n# Instead of latent_prefix → embed tokens → noising\nx0 = rwkv_token_embedder(tokens)  # [B,N,vocab] → [B,N,D=1024]\nxt = sqrt(alphabar_t) * x0 + sqrt(1-alphabar_t) * noise  # [B,N,D]\n# RWKV consumes xt directly (no prefix needed)\n```\n**Questions**:\n1. Embedding drift: How much does continuous diffusion move embeddings off token manifold?\n2. MSE vs hybrid loss: `||pred_eps - eps||^2 + λ * cross_entropy(pred_x0 @ E^T, tokens)`\n3. Comparison: Prefix (current) vs full embedding diffusion on same Qwen32d dataset\n\n### Prompt 5 — Training Stabilization Cookbook\n**Based on current `improved_trainer.py`**:\n```yaml\n# Add to configs/rwkv_mmdit.yaml\ntraining:\n  noise_schedule: cosine  # or linear\n  noise_levels: [0.0001, 0.999]  # curriculum\n  grad_clip: 1.0\n  backbone_unfreeze_epoch: 5  # gradual unfreezing\n  auxiliary_ce_weight: 0.1  # token alignment\n```\n**Deliverables**: Complete recipe validated on `data_simple.py` + expected convergence curves\n\n## Priority 4: Evaluation & Baselines\n\n### Prompt 6 — Concrete Metrics Dashboard\n**Extend `eval_checklist.md`**:\n```\n[ ] Latent reconstruction: MSE(x0_pred, x0) on Qwen32d val\n[ ] Text generation: Nucleus sampling from denoised embeddings → BLEU/MAUVE\n[ ] RWKV retention: Perplexity on AR text (test if RWKV language skill preserved)\n[ ] Speed: ms/step @ N=512, T=50 diffusion steps vs DiT baseline\n[ ] Block size ablation: B∈{32,64,128} → quality/speed Pareto\n```\n\n## Engineering Tasks (Implementation-Ready)\n\n### Prompt 7 — MM-DiT Integration Plan\n**Files to modify**:\n1. `latentDLM_mmdit/train_mmdit.py` → accept `denoiser_type: rwkv`\n2. `models/rwkv_denoiser.py` → register as pluggable denoiser\n3. `configs/` → `rwkv_block_causal.yaml`\n\n**Minimal diff**:\n```diff\n# train_mmdit.py\n+ if cfg.denoiser_type == \"rwkv\":\n+     denoiser = MultimodalRWKVDenoiser(...)\n-     denoiser = DiTBlock(...)\n```\n\n---\n\n**Next**: Which priority should we tackle first? **Block-causal implementation (Prompt 1)** recommended as highest impact.","file_path":"/Users/joshuazhang/Desktop/FDU/DLM/MM-LDLM/rwkv_diffusion_rnn/DiR_further_research_prompts.md"}
</xai:function_call<xai:function_call name="TodoWrite">
<parameter name="todos">[{"activeForm":"Analyzing existing DiR report and research prompts","content":"Analyze existing DiR report and research prompts in detail","status":"completed"},{"activeForm":"Identifying key areas needing elaboration","content":"Identify key areas needing elaboration in the DiR report","status":"completed"},{"activeForm":"Expanding on causality adaptation strategies","content":"Expand on the three causality adaptation strategies","status":"in_progress"},{"activeForm":"Developing detailed implementation guidelines","content":"Develop detailed implementation guidelines for block-causal RWKV","status":"pending"},{"activeForm":"Creating enhanced research prompts","content":"Create enhanced research prompts with concrete examples","status":"completed"},{"activeForm":"Compiling comprehensive appendix","content":"Compile a comprehensive appendix with mathematical formulations","status":"pending"}]