# Stage II Variational Inference Implementation

## Overview

This document describes the Stage II implementation for the RWKV diffusion model, which adds variational inference capabilities on top of the Stage I coupling framework.

## Architecture

### Two-Stage Training Pipeline

**Stage I: Coupling (Diffusion-based)**
- Joint text-latent diffusion training
- Learns bidirectional mappings: text ↔ latent
- Uses external pre-computed latents from E5/other encoders
- Mode: `stage1_coupling` (formerly `unconditional`)

**Stage II: Variational Inference**
- Adds variational encoder q(z|x) and prior p(z)
- Learns to generate latents from text without external encoders
- Enables end-to-end text generation via latent sampling
- Supports partial fine-tuning of Stage I checkpoint

## New Components

### 1. Configuration Fields (`configs/rwkv_mmdit.yaml`)

```yaml
model:
  latent_pred_from: "text"  # Options: text, prefix, both
  latent_encoder_type: "mlp"  # For Stage II
  latent_prior_type: "fixed_gaussian"  # Options: fixed_gaussian, learned_gaussian

data:
  use_external_latents: true  # Stage I: true, Stage II: false

training:
  stage_schedule: "stage1_coupling"  # or "stage2_variational"
  unfreeze_after_steps: 5000  # Partial fine-tuning threshold
  unfreeze_patterns:  # Regex patterns for unfreezing
    - "rwkv.*time_mix"
    - "rwkv.*channel_mix"

loss:
  kl_weight: 0.001  # KL divergence weight for Stage II
  geom_weight: 0.0  # Geometric regularization weight
```

### 2. T2L Conditioning (`models/rwkv_denoiser.py`)

Added `latent_pred_from` parameter to control how text conditions latent generation:

- `text`: Use final RWKV hidden states
- `prefix`: Use only prefix token embeddings
- `both`: Concatenate prefix + final hidden states

### 3. Variational Modules (`models/latent_modules.py`)

**TextLatentEncoder**
- Encodes text hidden states to latent distribution parameters (mu, logvar)
- 2-layer MLP with GELU activation
- Separate heads for mu and logvar

**LatentPrior**
- Defines p(z) for KL divergence computation
- `fixed_gaussian`: Standard normal N(0, I)
- `learned_gaussian`: Learnable mu and logvar parameters

**Utility Functions**
- `kl_divergence()`: Computes KL(q||p)
- `reparameterize()`: Samples z ~ q(z|x) using reparameterization trick

### 4. Partial Fine-Tuning (`improved_trainer.py`)

**`_apply_unfreeze_policy()`**
- Unfreezes parameters after specified step count
- Uses regex patterns to selectively unfreeze layers
- Enables gradual adaptation of Stage I checkpoint

**Parameter Tracking**
- Logs active parameter counts every 100 steps
- Verifies freezing correctness for each training mode
- Reports parameter efficiency metrics

### 5. Data Pipeline Updates (`data_simple.py`)

**DirectFileDataset**
- Added `use_external_latents` parameter
- Stage I: loads pre-computed latents from .npy files
- Stage II: returns None for latents (generated by encoder)

**Collate Function**
- Handles optional latent tensors
- Creates zero tensors when latents unavailable

### 6. Sampling Functions (`sampling.py`)

**`sample_text_from_latent()`**
- L2T generation: latent → text
- Supports temperature, top-k, top-p sampling
- Iterative denoising over num_steps

**`sample_latent_from_text()`**
- T2L generation: text → latent
- Supports classifier-free guidance
- Returns latent representation

**`sample_joint()`**
- Unconditional joint generation
- Simultaneously denoises text and latent
- Returns both modalities

## Usage

### Stage I Training (Coupling)

```bash
python rwkv_diffusion_rnn/train_rwkv.py \
  --config-name rwkv_mmdit \
  model.latent_pred_from=text \
  data.use_external_latents=true \
  data.token_dir=preprocessed_data/tokens \
  data.latent_dir=preprocessed_data/latents \
  training.stage_schedule=stage1_coupling \
  training.loss_type=stage1_coupling \
  training.num_epochs=10
```

### Stage II Training (Variational)

```bash
python rwkv_diffusion_rnn/train_rwkv.py \
  --config-name rwkv_mmdit \
  model.latent_encoder_type=mlp \
  model.latent_prior_type=fixed_gaussian \
  data.use_external_latents=false \
  training.stage_schedule=stage2_variational \
  training.unfreeze_after_steps=5000 \
  training.unfreeze_patterns='["rwkv.*time_mix"]' \
  loss.kl_weight=0.001 \
  training.num_epochs=5 \
  model.checkpoint_path=outputs/stage1/checkpoint.pt
```

### Conditional Training Modes

**L2T (Latent-to-Text)**
```bash
training.loss_type=l2t
```

**T2L (Text-to-Latent)**
```bash
training.loss_type=t2l
```

**Sequential Training**
```bash
training.loss_type=sequential
training.sequential_schedule='[
  {"type": "stage1_coupling", "steps": 10000},
  {"type": "l2t", "steps": 5000},
  {"type": "t2l", "steps": 5000}
]'
```

### Inference

```python
from rwkv_diffusion_rnn.sampling import (
    sample_text_from_latent,
    sample_latent_from_text,
    sample_joint
)

# L2T: Generate text from latent
latent = torch.randn(1, 1, 1024)
text = sample_text_from_latent(
    model, latent, seq_len=512,
    num_steps=100, temperature=0.9
)

# T2L: Generate latent from text
input_ids = tokenizer.encode("Hello world")
latent = sample_latent_from_text(
    model, input_ids, latent_dim=1024,
    num_steps=100, guidance_scale=1.5
)

# Joint: Generate both
text, latent = sample_joint(
    model, batch_size=1, seq_len=512,
    latent_dim=1024, num_steps=100
)
```

## Evaluation

See `eval_checklist.md` for comprehensive evaluation procedures:

**Stage I Checks**
- Forward pass shape verification
- Loss finiteness for all modes
- Latent retention and norm monitoring
- Parameter freezing verification

**Stage II Checks**
- KL divergence monitoring (avoid collapse)
- Encoder mu/logvar validity
- Reparameterization stochasticity
- Reconstruction quality
- Latent space geometry

## Key Design Decisions

1. **Two-stage approach**: Stage I learns strong text-latent coupling with external latents, Stage II adds variational inference for end-to-end generation

2. **Partial fine-tuning**: Stage II can selectively unfreeze Stage I parameters to adapt without catastrophic forgetting

3. **Flexible conditioning**: `latent_pred_from` allows experimenting with different text representations for T2L

4. **Optional external latents**: Data pipeline supports both pre-computed (Stage I) and encoder-generated (Stage II) latents

5. **Mode-specific parameter freezing**: Efficient training by freezing unused parameters in L2T/T2L modes

## Next Steps

1. **Hyperparameter tuning**: Adjust `kl_weight`, `unfreeze_after_steps`, learning rates
2. **Encoder architecture**: Experiment with transformer-based encoders vs MLP
3. **Prior learning**: Try learned Gaussian prior vs fixed
4. **Geometric regularization**: Enable `geom_weight` for structured latent space
5. **Multi-stage scheduling**: Combine stage1_coupling → l2t → t2l → stage2_variational

## Files Modified

- `configs/rwkv_mmdit.yaml`: Added Stage II config fields
- `models/rwkv_denoiser.py`: Added `latent_pred_from` parameter
- `improved_trainer.py`: Added partial fine-tuning policy
- `data_simple.py`: Added `use_external_latents` toggle
- `eval_checklist.md`: Expanded with Stage II verification

## Files Created

- `models/latent_modules.py`: Variational inference components
- `sampling.py`: Inference functions for L2T/T2L/joint generation
- `STAGE2_IMPLEMENTATION.md`: This document
